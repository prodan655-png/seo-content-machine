import os
from utils.ai_handler import AIHandler
from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup
import json
import requests
import time
import random

class Strategist:
    def __init__(self, api_key):
        self.ai_handler = AIHandler(api_key, model_name="gemini-2.5-flash")

    # ... (previous methods) ...

    def analyze_competitors(self, urls):
        """
        Scrapes competitor URLs to extract outlines (H1-H3).
        """
        outlines = []
        
        # Try Playwright first
        try:
            with sync_playwright() as p:
                try:
                    browser = p.chromium.launch(headless=True)
                except Exception as e:
                    # print(f"Playwright launch failed: {e}. Installing browsers...")
                    pass
                    os.system("playwright install chromium")
                    browser = p.chromium.launch(headless=True)
                
                for url in urls:
                    try:
                        page = browser.new_page()
                        page.goto(url, timeout=15000)
                        content = page.content()
                        soup = BeautifulSoup(content, 'html.parser')
                        
                        h1 = soup.find('h1').get_text().strip() if soup.find('h1') else "No H1"
                        headings = [h.get_text().strip() for h in soup.find_all(['h2', 'h3'])]
                        
                        outlines.append({
                            "url": url,
                            "h1": h1,
                            "structure": headings[:10] # Limit to top 10 headings
                        })
                        page.close()
                    except Exception as e:
                        # print(f"Playwright failed for {url}: {e}")
                        # Fallback to Requests inside loop
                        self._scrape_fallback(url, outlines)
                browser.close()
                
        except Exception as e:
            # print(f"Playwright crashed completely: {e}")
            # Fallback for all URLs if browser fails entirely
            for url in urls:
                 self._scrape_fallback(url, outlines)

        return outlines

    def _scrape_fallback(self, url, outlines_list):
        """Fallback scraper using requests."""
        try:
            # print(f"Falling back to requests for {url}")
            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
            response = requests.get(url, headers=headers, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            h1 = soup.find('h1').get_text().strip() if soup.find('h1') else "No H1 (Requests)"
            headings = [h.get_text().strip() for h in soup.find_all(['h2', 'h3'])]
            
            outlines_list.append({
                "url": url,
                "h1": h1,
                "structure": headings[:10]
            })
        except Exception as e:
            # print(f"Requests fallback failed for {url}: {e}")
            outlines_list.append({
                "url": url,
                "h1": "Error scraping",
                "structure": [f"Error: {str(e)}"]
            })

    def generate_topic_ideas(self, niche, num_topics=10, context_data=None):
        """
        Generates topic ideas for a given niche, optionally using context (competitors/sitemap).
        
        Args:
            niche: The niche/industry
            num_topics: Number of topics
            context_data: Optional string containing competitor headers or sitemap titles
        """
        context_prompt = ""
        if context_data:
            context_prompt = f"Based on the following competitor/existing content analysis:\n{context_data[:5000]}\n\n"

        prompt = f"""
        You are an SEO content strategist. Generate {num_topics} article topic ideas for the niche: "{niche}".
        
        {context_prompt}
        
        Requirements:
        - Topics should be SEO-friendly and address user search intent
        - Mix informational, commercial, and transactional intents
        - Include long-tail keywords where appropriate
        - Make titles compelling and click-worthy
        - Write in Ukrainian language
        
        Return as JSON array with format:
        [
            {{"title": "Topic Title", "description": "Brief description of what the article would cover"}}
        ]
        
        JSON ONLY. NO MARKDOWN.
        """
        
        try:
            response = self.ai_handler.generate_content(prompt)
            topics = json.loads(response.text.replace('```json', '').replace('```', ''))
            return topics if isinstance(topics, list) else []
        except Exception as e:
            return [{"title": f"–¢–µ–º–∞ {i+1} –¥–ª—è {niche}", "description": "–û–ø–∏—Å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π"} for i in range(num_topics)]

    def generate_keywords(self, topic, num_keywords=20):
        """
        Generates SEO keywords for a topic.
        """
        prompt = f"""
        Generate {num_keywords} SEO keywords for the topic: "{topic}".
        Include a mix of:
        - Head terms (high volume)
        - Long-tail keywords (specific intent)
        - LSI keywords (semantically related)
        
        Return as a JSON list of objects:
        [
            {{"keyword": "keyword 1", "type": "Head"}},
            {{"keyword": "keyword 2", "type": "Long-tail"}},
            ...
        ]
        
        Language: Ukrainian.
        JSON ONLY.
        """
        try:
            response = self.ai_handler.generate_content(prompt)
            return json.loads(response.text.replace('```json', '').replace('```', ''))
        except:
            return [{"keyword": topic, "type": "Head"}]




    def analyze_serp(self, topic):
        """
        Analyzes SERP for intent and features.
        """
        # print(f"Analyzing SERP for: {topic}")
        results = []
        
        # Attempt 1: Playwright (Google) - Stealth Mode
        try:
            with sync_playwright() as p:
                try:
                    browser = p.chromium.launch(headless=True, args=["--disable-blink-features=AutomationControlled"])
                except Exception as e:
                    # print(f"Playwright launch failed: {e}. Installing browsers...")
                    os.system("playwright install chromium")
                    browser = p.chromium.launch(headless=True, args=["--disable-blink-features=AutomationControlled"])
                
                context = browser.new_context(
                    user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
                )
                page = context.new_page()
                
                # print(f"Attempt 1: Scraping Google for: {topic}")
                try:
                    page.goto(f"https://www.google.com/search?q={topic}", timeout=10000)
                    page.wait_for_load_state("domcontentloaded")
                    
                    links = page.query_selector_all('div.g h3') # Standard Google selector
                    for h3 in links[:5]:
                        try:
                            parent_a = h3.xpath('..')[0]
                            url = parent_a.get_attribute('href')
                            title = h3.inner_text()
                            if url and title and url.startswith('http'):
                                results.append({"url": url, "title": title})
                        except:
                            continue
                except Exception as e:
                    # print(f"Google scrape failed: {e}")
                    pass

                browser.close()
        except Exception as e:
            # print(f"Playwright Google failed: {e}")
            pass

        # Attempt 2: Requests (DuckDuckGo HTML) - Very Robust Fallback
        if not results:
            # print("Attempt 2: Using DuckDuckGo HTML (Requests)...")
            try:
                import requests
                headers = {
                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
                    "Referer": "https://duckduckgo.com/"
                }
                # DDG HTML version doesn't require JS
                resp = requests.post("https://html.duckduckgo.com/html/", data={'q': topic}, headers=headers, timeout=10)
                if resp.status_code == 200:
                    soup = BeautifulSoup(resp.text, 'html.parser')
                    # DDG HTML selectors
                    for link in soup.find_all('a', class_='result__a')[:5]:
                        url = link.get('href')
                        title = link.get_text(strip=True)
                        if url and title:
                            results.append({"url": url, "title": title})
            except Exception as e:
                # print(f"DDG Requests failed: {e}")
                pass

        # Attempt 3: Simulation (Last Resort)
        if not results:
            # print("All scraping failed. Returning empty list to trigger manual input or retry.")
            # Do NOT return fake data anymore, it confuses the user.
            # Let's return a specific error-like result so the UI can show a warning but not fake info.
            results = [] 


        # Analyze Intent with Gemini
        prompt = f"Analyze the search intent for the topic '{topic}' in the context of Ukrainian Google Search. Return JSON with keys: 'intent' (Informational/Commercial/Transactional - translate to Ukrainian), 'features' (list of likely SERP features e.g. '–í—ñ–¥–µ–æ', '–°–Ω—ñ–ø–µ—Ç', '–ö–∞—Ä—Ç–∏–Ω–∫–∏')."
        response = self.ai_handler.generate_content(prompt)
        try:
            analysis = json.loads(response.text.replace('```json', '').replace('```', ''))
        except:
            analysis = {"intent": "Informational", "features": []}

        return {
            "topic": topic,
            "competitors": results,
            "intent": analysis.get("intent"),
            "serp_features": analysis.get("features")
        }

    def analyze_competitors(self, urls):
        """
        Scrapes competitor URLs to extract outlines (H1-H3).
        """
        outlines = []
        with sync_playwright() as p:
            try:
                browser = p.chromium.launch(headless=True)
            except Exception as e:
                # print(f"Playwright launch failed: {e}. Installing browsers...")
                os.system("playwright install chromium")
                browser = p.chromium.launch(headless=True)
            for url in urls:
                try:
                    page = browser.new_page()
                    page.goto(url, timeout=15000)
                    content = page.content()
                    soup = BeautifulSoup(content, 'html.parser')
                    
                    h1 = soup.find('h1').get_text(strip=True) if soup.find('h1') else "No H1"
                    headings = []
                    for h in soup.find_all(['h2', 'h3']):
                        headings.append(f"{h.name.upper()}: {h.get_text(strip=True)}")
                    
                    outlines.append({
                        "url": url,
                        "h1": h1,
                        "structure": headings[:10] # Limit for brevity
                    })
                    page.close()
                except Exception as e:
                    # print(f"Error scraping {url}: {e}")
                    pass
            browser.close()
        return outlines

    def extract_entities(self, text_content):
        """
        Uses Gemini to extract entities.
        """
        prompt = f"Extract key entities (products, ingredients, brands, technical terms) from the following text. Return as a JSON list of strings.\n\nText: {text_content[:2000]}"
        response = self.ai_handler.generate_content(prompt)
        try:
            return json.loads(response.text.replace('```json', '').replace('```', ''))
        except:
            return []

    def suggest_faq(self, topic):
        """
        Generates FAQ questions based on the topic.
        """
        prompt = f"Generate 5 relevant FAQ questions for the topic '{topic}' that users might ask on Google. Return as a JSON list of strings."
        response = self.ai_handler.generate_content(prompt)
        try:
            return json.loads(response.text.replace('```json', '').replace('```', ''))
        except:
            return [f"What is {topic}?", f"How to use {topic}?"]

    def generate_tov(self, brand_name, industry, url=None, emotional_tone="–ù–µ–π—Ç—Ä–∞–ª—å–Ω–∏–π", formality_level="–ù–µ–π—Ç—Ä–∞–ª—å–Ω–∏–π", unique_trait="", uploaded_docs=None):
        """
        Generates a Tone of Voice description based on brand info and optional URL scraping.
        """
        context = ""
        if url:
            try:
                with sync_playwright() as p:
                    browser = p.chromium.launch(headless=True)
                    page = browser.new_page()
                    page.goto(url, timeout=15000)
                    # Get text from home page
                    text = page.inner_text('body')[:5000] # Limit text
                    context = f"–ö–æ–Ω—Ç–µ–Ω—Ç —Å–∞–π—Ç—É:\n{text}\n\n"
                    browser.close()
            except Exception as e:
                # print(f"ToV Scraping failed: {e}")
                context = "Could not scrape website. "
        
        # Add uploaded documents context
        if uploaded_docs:
            # No need to import parser here, text is already extracted in app.py
            docs_context = "\n\n–î–æ–¥–∞—Ç–∫–æ–≤—ñ –º–∞—Ç–µ—Ä—ñ–∞–ª–∏ –ø—Ä–æ –±—Ä–µ–Ω–¥:\n"
            for doc in uploaded_docs[:3]:  # Limit to 3 docs
                try:
                    # Check if content is already text (new optimization) or bytes (old way)
                    if doc.get('is_text', False):
                        text = doc['content']
                    else:
                        # Fallback for backward compatibility
                        from utils.document_parser import extract_text_from_document
                        text = extract_text_from_document(doc['content'], doc['type'])
                        
                    docs_context += f"\n--- {doc['name']} ---\n{text[:5000]}\n"  # Increased limit to 5000 chars
                except Exception as e:
                    # print(f"Error parsing {doc['name']}: {e}")
                    pass
            context += docs_context

        prompt = f"""
        –¢–∏ - –µ–∫—Å–ø–µ—Ä—Ç –∑ –±—Ä–µ–Ω–¥–∏–Ω–≥—É. –°—Ç–≤–æ—Ä–∏ –¥–µ—Ç–∞–ª—å–Ω–∏–π –≥–∞–π–¥ Tone of Voice (–ì–æ–ª–æ—Å –ë—Ä–µ–Ω–¥—É) –¥–ª—è –±—Ä–µ–Ω–¥—É.
        
        –í–ê–ñ–õ–ò–í–û: –í—Å—è –≤—ñ–¥–ø–æ–≤—ñ–¥—å –û–ë–û–í'–Ø–ó–ö–û–í–û –º–∞—î –±—É—Ç–∏ –£–ö–†–ê–á–ù–°–¨–ö–û–Æ –º–æ–≤–æ—é!
        
        –Ü–ù–§–û–†–ú–ê–¶–Ü–Ø –ü–†–û –ë–†–ï–ù–î:
        - –ù–∞–∑–≤–∞ –±—Ä–µ–Ω–¥—É: {brand_name}
        - –Ü–Ω–¥—É—Å—Ç—Ä—ñ—è: {industry}
        - –ï–º–æ—Ü—ñ–π–Ω–∏–π —Ç–æ–Ω: {emotional_tone}
        - –†—ñ–≤–µ–Ω—å —Ñ–æ—Ä–º–∞–ª—å–Ω–æ—Å—Ç—ñ: {formality_level}
        - –£–Ω—ñ–∫–∞–ª—å–Ω–∞ —Ä–∏—Å–∞: {unique_trait if unique_trait else "–Ω–µ –≤–∫–∞–∑–∞–Ω–æ"}
        {context}
        
        –°—Ç–≤–æ—Ä–∏ Markdown –≥–∞–π–¥, —è–∫–∏–π –≤–∫–ª—é—á–∞—î:
        
        ## 1. –û—Å–Ω–æ–≤–Ω—ñ —Ü—ñ–Ω–Ω–æ—Å—Ç—ñ
        - 3-5 –∫–ª—é—á–æ–≤–∏—Ö —Ü—ñ–Ω–Ω–æ—Å—Ç–µ–π –±—Ä–µ–Ω–¥—É (–º–∞—Ä–∫–æ–≤–∞–Ω–∏–π —Å–ø–∏—Å–æ–∫)
        
        ## 2. –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –≥–æ–ª–æ—Å—É
        - 5-7 –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∏—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ (–º–∞—Ä–∫–æ–≤–∞–Ω–∏–π —Å–ø–∏—Å–æ–∫)
        - –í—Ä–∞—Ö–æ–≤—É–π –æ–±—Ä–∞–Ω–∏–π –µ–º–æ—Ü—ñ–π–Ω–∏–π —Ç–æ–Ω ({emotional_tone}) —Ç–∞ —Ñ–æ—Ä–º–∞–ª—å–Ω—ñ—Å—Ç—å ({formality_level})
        
        ## 3. –©–æ —Ä–æ–±–∏—Ç–∏
        - 5+ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∏—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ–π –¥–ª—è –∫–æ–º—É–Ω—ñ–∫–∞—Ü—ñ—ó
        
        ## 4. –ß–æ–≥–æ –ù–ï —Ä–æ–±–∏—Ç–∏ (–ß–µ—Ä–≤–æ–Ω—ñ –ø—Ä–∞–ø–æ—Ä—Ü—ñ)
        - 5+ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∏—Ö —Ä–µ—á–µ–π, —è–∫–∏—Ö —Å–ª—ñ–¥ —É–Ω–∏–∫–∞—Ç–∏
        - –§—Ä–∞–∑–∏, —è–∫—ñ –ù–Ü–ö–û–õ–ò –Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏
        
        ## 5. –ü—Ä–∏–∫–ª–∞–¥–∏ —Ñ—Ä–∞–∑ (–º—ñ–Ω—ñ–º—É–º 10)
        –°—Ç–≤–æ—Ä–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ñ –ø—Ä–∏–∫–ª–∞–¥–∏ –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö —Å–∏—Ç—É–∞—Ü—ñ–π:
        - –ó–∞–≥–æ–ª–æ–≤–∫–∏ –ø—Ä–æ–¥—É–∫—Ç—ñ–≤ (2 –ø—Ä–∏–∫–ª–∞–¥–∏)
        - –û–ø–∏—Å–∏ –∫–∞—Ç–µ–≥–æ—Ä—ñ–π (2 –ø—Ä–∏–∫–ª–∞–¥–∏)
        - Email —Ä–æ–∑—Å–∏–ª–∫–∏ (2 –ø—Ä–∏–∫–ª–∞–¥–∏)
        - –°–æ—Ü–º–µ—Ä–µ–∂—ñ (2 –ø—Ä–∏–∫–ª–∞–¥–∏)
        - –û–±—Ä–æ–±–∫–∞ –∑–∞–ø–µ—Ä–µ—á–µ–Ω—å (2 –ø—Ä–∏–∫–ª–∞–¥–∏)
        
        –ö–æ–∂–µ–Ω –ø—Ä–∏–∫–ª–∞–¥ –º–∞—î –±—É—Ç–∏ —É —Ñ–æ—Ä–º–∞—Ç—ñ:
        **–°–∏—Ç—É–∞—Ü—ñ—è:** [–æ–ø–∏—Å]
        **–§—Ä–∞–∑–∞:** "[–∫–æ–Ω–∫—Ä–µ—Ç–Ω–∞ —Ñ—Ä–∞–∑–∞ —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é]"
        
        –ö–†–ò–¢–ò–ß–ù–û –í–ê–ñ–õ–ò–í–û: 
        - –ù–ï –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π —Ç–∞–±–ª–∏—Ü—ñ! –¢—ñ–ª—å–∫–∏ –º–∞—Ä–∫–æ–≤–∞–Ωi —Å–ø–∏—Å–∫–∏.
        - –í—Å—è –≤—ñ–¥–ø–æ–≤—ñ–¥—å –º–∞—î –±—É—Ç–∏ –£–ö–†–ê–á–ù–°–¨–ö–û–Æ –º–æ–≤–æ—é (–≤–∫–ª—é—á–Ω–æ –∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏)!
        - –ë—É–¥—å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∏–º, –Ω–µ –∑–∞–≥–∞–ª—å–Ω–∏–º.
        - –í—Ä–∞—Ö–æ–≤—É–π —É–Ω—ñ–∫–∞–ª—å–Ω—É —Ä–∏—Å—É –±—Ä–µ–Ω–¥—É: {unique_trait if unique_trait else "—Å—Ç–≤–æ—Ä–∏ —É–Ω—ñ–∫–∞–ª—å–Ω–∏–π —Å—Ç–∏–ª—å"}
        - –Ø–∫—â–æ —î –¥–æ–¥–∞—Ç–∫–æ–≤—ñ –º–∞—Ç–µ—Ä—ñ–∞–ª–∏, –û–ë–û–í'–Ø–ó–ö–û–í–û –ø–æ—Å–∏–ª–∞–π—Å—è –Ω–∞ –Ω–∏—Ö (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥: "–ó–≥—ñ–¥–Ω–æ –∑ –≤–∞—à–æ—é —Å—Ç—Ä–∞—Ç–µ–≥—ñ—î—é...", "–Ø–∫ –∑–∞–∑–Ω–∞—á–µ–Ω–æ –≤ –¥–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—ñ...").
        - –Ø–∫—â–æ –º–∞—Ç–µ—Ä—ñ–∞–ª–∏ —Å—É–ø–µ—Ä–µ—á–∞—Ç—å –æ–¥–∏–Ω –æ–¥–Ω–æ–º—É, –Ω–∞–¥–∞–≤–∞–π –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∏–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º.
        """
        
        # print(f"Generating ToV for {brand_name}...")
        response = self.ai_handler.generate_content(prompt)
        # print(f"ToV Generated (Length: {len(response.text)})")
        return response.text

    def refine_tov(self, current_tov, instructions):
        """
        Refines the existing ToV based on user instructions.
        """
        prompt = f"""
        Act as a Brand Strategist. Refine the following Tone of Voice guide based on the user's instructions.
        
        Current ToV:
        {current_tov}
        
        User Instructions:
        {instructions}
        
        Output the updated ToV in Markdown. Keep the same structure if possible, but apply the requested changes. Language: Ukrainian.
        """
        
        # print(f"Refining ToV...")
        response = self.ai_handler.generate_content(prompt)
        # print(f"ToV Refined (Length: {len(response.text)})")
        return response.text

    def generate_audience(self, brand_name, industry, url=None, business_model="B2C", num_personas=2):
        """
        Generates detailed target audience personas with Jobs-to-be-Done framework.
        """
        context = ""
        if url:
            try:
                with sync_playwright() as p:
                    browser = p.chromium.launch(headless=True)
                    page = browser.new_page()
                    page.goto(url, timeout=15000)
                    text = page.inner_text('body')[:5000]
                    context = f"–ö–æ–Ω—Ç–µ–Ω—Ç —Å–∞–π—Ç—É:\n{text}\n\n"
                    browser.close()
            except Exception as e:
                # print(f"Audience scraping failed: {e}")
                context = ""

        # Extract business model type
        if "B2B" in business_model and "B2C" in business_model:
            biz_type = "B2B —Ç–∞ B2C"
        elif "B2B" in business_model:
            biz_type = "B2B (–±—ñ–∑–Ω–µ—Å –¥–ª—è –±—ñ–∑–Ω–µ—Å—É)"
        else:
            biz_type = "B2C (–±—ñ–∑–Ω–µ—Å –¥–ª—è —Å–ø–æ–∂–∏–≤–∞—á—ñ–≤)"

        prompt = f"""
        –¢–∏ - –µ–∫—Å–ø–µ—Ä—Ç –∑ –º–∞—Ä–∫–µ—Ç–∏–Ω–≥—É. –°—Ç–≤–æ—Ä–∏ {num_personas} –¥–µ—Ç–∞–ª—å–Ω—ñ –ø–µ—Ä—Å–æ–Ω–∏ –¥–ª—è –±—Ä–µ–Ω–¥—É.
        
        –í–ê–ñ–õ–ò–í–û: –í—Å—è –≤—ñ–¥–ø–æ–≤—ñ–¥—å –û–ë–û–í'–Ø–ó–ö–û–í–û –º–∞—î –±—É—Ç–∏ –£–ö–†–ê–á–ù–°–¨–ö–û–Æ –º–æ–≤–æ—é!
        
        –Ü–ù–§–û–†–ú–ê–¶–Ü–Ø –ü–†–û –ë–†–ï–ù–î:
        - –ù–∞–∑–≤–∞ –±—Ä–µ–Ω–¥—É: {brand_name}
        - –Ü–Ω–¥—É—Å—Ç—Ä—ñ—è: {industry}
        - –¢–∏–ø –±—ñ–∑–Ω–µ—Å—É: {biz_type}
        {context}
        
        –°—Ç–≤–æ—Ä–∏ {num_personas} –†–Ü–ó–ù–Ü –ø–µ—Ä—Å–æ–Ω–∏. –ö–æ–∂–Ω–∞ –ø–µ—Ä—Å–æ–Ω–∞ –º–∞—î –±—É—Ç–∏ –£–ù–Ü–ö–ê–õ–¨–ù–û–Æ —Ç–∞ –ö–û–ù–ö–†–ï–¢–ù–û–Æ.
        
        –î–õ–Ø –ö–û–ñ–ù–û–á –ü–ï–†–°–û–ù–ò —Å—Ç–≤–æ—Ä–∏ —Ç–∞–∫–∏–π Markdown —Ä–æ–∑–¥—ñ–ª:
        
        ## –ü–µ—Ä—Å–æ–Ω–∞ [–ù–æ–º–µ—Ä]: [–£–∫—Ä–∞—ó–Ω—Å—å–∫–µ —ñ–º'—è] - [–†–æ–ª—å/–ü–æ—Å–∞–¥–∞]
        
        ### üë§ –î–µ–º–æ–≥—Ä–∞—Ñ—ñ—è
        - **–í—ñ–∫:** [–∫–æ–Ω–∫—Ä–µ—Ç–Ω–∏–π –¥—ñ–∞–ø–∞–∑–æ–Ω]
        - **–°—Ç–∞—Ç—å:** [—Å—Ç–∞—Ç—å]
        - **–õ–æ–∫–∞—Ü—ñ—è:** [–º—ñ—Å—Ç–æ/—Ä–µ–≥—ñ–æ–Ω –£–∫—Ä–∞—ó–Ω–∏]
        - **–û—Å–≤—ñ—Ç–∞:** [—Ä—ñ–≤–µ–Ω—å –æ—Å–≤—ñ—Ç–∏]
        - **–î–æ—Ö—ñ–¥:** [—Ä—ñ–≤–µ–Ω—å –¥–æ—Ö–æ–¥—É]
        - **–°—ñ–º–µ–π–Ω–∏–π —Å—Ç–∞–Ω:** [—Å—Ç–∞–Ω]
        
        ### üß† –ü—Å–∏—Ö–æ–≥—Ä–∞—Ñ—ñ—è
        - **–¶—ñ–Ω–Ω–æ—Å—Ç—ñ:** [3-4 –∫–ª—é—á–æ–≤—ñ —Ü—ñ–Ω–Ω–æ—Å—Ç—ñ]
        - **–Ü–Ω—Ç–µ—Ä–µ—Å–∏:** [—Ö–æ–±—ñ, –∑–∞—Ö–æ–ø–ª–µ–Ω–Ω—è]
        - **–°—Ç–∏–ª—å –∂–∏—Ç—Ç—è:** [–æ–ø–∏—Å –ø–æ–≤—Å—è–∫–¥–µ–Ω–Ω–æ–≥–æ –∂–∏—Ç—Ç—è]
        - **–£–ª—é–±–ª–µ–Ω—ñ –±—Ä–µ–Ω–¥–∏:** [2-3 –±—Ä–µ–Ω–¥–∏, —è–∫—ñ –≤–æ–Ω–∏ –ª—é–±–ª—è—Ç—å]
        - **–ú–µ–¥—ñ–∞:** [–¥–µ —à—É–∫–∞—é—Ç—å —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é - —Å–æ—Ü–º–µ—Ä–µ–∂—ñ, –±–ª–æ–≥–∏, YouTube —Ç–æ—â–æ]
        
        ### üéØ Jobs-to-be-Done
        - **–§—É–Ω–∫—Ü—ñ–æ–Ω–∞–ª—å–Ω–∞ —Ä–æ–±–æ—Ç–∞:** [—â–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ —Ö–æ—á—É—Ç—å –∑—Ä–æ–±–∏—Ç–∏/–¥–æ—Å—è–≥—Ç–∏]
        - **–ï–º–æ—Ü—ñ–π–Ω–∞ —Ä–æ–±–æ—Ç–∞:** [—è–∫ —Ö–æ—á—É—Ç—å –≤—ñ–¥—á—É–≤–∞—Ç–∏ —Å–µ–±–µ]
        - **–°–æ—Ü—ñ–∞–ª—å–Ω–∞ —Ä–æ–±–æ—Ç–∞:** [—è–∫ —Ö–æ—á—É—Ç—å –≤–∏–≥–ª—è–¥–∞—Ç–∏ –≤ –æ—á–∞—Ö —ñ–Ω—à–∏—Ö]
        
        ### üíî –ë–æ–ª—ñ —Ç–∞ –±–∞—Ä'—î—Ä–∏
        - [–ë—ñ–ª—å 1: –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∞ –ø—Ä–æ–±–ª–µ–º–∞]
        - [–ë—ñ–ª—å 2: –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∞ –ø—Ä–æ–±–ª–µ–º–∞]
        - [–ë—ñ–ª—å 3: –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∞ –ø—Ä–æ–±–ª–µ–º–∞]
        - **–ì–æ–ª–æ–≤–Ω–∏–π –±–∞—Ä'—î—Ä –¥–æ –ø–æ–∫—É–ø–∫–∏:** [—â–æ –∑–∞–≤–∞–∂–∞—î –∫—É–ø–∏—Ç–∏]
        
        ### üöÄ –¢—Ä–∏–≥–µ—Ä–∏ —Ç–∞ –º–æ—Ç–∏–≤–∞—Ç–æ—Ä–∏
        - **–©–æ –∑–º—É—à—É—î —à—É–∫–∞—Ç–∏ —Ä—ñ—à–µ–Ω–Ω—è:** [–∫–æ–Ω–∫—Ä–µ—Ç–Ω–∞ —Å–∏—Ç—É–∞—Ü—ñ—è]
        - **–ö—Ä–∏—Ç–µ—Ä—ñ—ó –≤–∏–±–æ—Ä—É:** [—Ç–æ–ø-3 –∫—Ä–∏—Ç–µ—Ä—ñ—ó –ø—Ä–∏ –≤–∏–±–æ—Ä—ñ –ø—Ä–æ–¥—É–∫—Ç—É]
        - **–©–æ –≤–ø–ª–∏–≤–∞—î –Ω–∞ —Ä—ñ—à–µ–Ω–Ω—è:** [—Ö—Ç–æ/—â–æ –≤–ø–ª–∏–≤–∞—î –Ω–∞ –≤–∏–±—ñ—Ä]
        
        ### üõí Customer Journey
        - **–£—Å–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è:** [—è–∫ –¥—ñ–∑–Ω–∞—î—Ç—å—Å—è –ø—Ä–æ –ø—Ä–æ–±–ª–µ–º—É]
        - **–†–æ–∑–≥–ª—è–¥:** [—è–∫ –ø–æ—Ä—ñ–≤–Ω—é—î –≤–∞—Ä—ñ–∞–Ω—Ç–∏, —â–æ –≥—É–≥–ª–∏—Ç—å]
        - **–†—ñ—à–µ–Ω–Ω—è:** [—â–æ –æ—Å—Ç–∞—Ç–æ—á–Ω–æ –≤–ø–ª–∏–≤–∞—î –Ω–∞ –≤–∏–±—ñ—Ä]
        - **–ü–æ–∫—É–ø–∫–∞:** [–¥–µ —Ç–∞ —è–∫ –∫—É–ø—É—î]
        
        ### üí¨ –¶–∏—Ç–∞—Ç–∞ –ø–µ—Ä—Å–æ–Ω–∏
        "[–†–µ–∞–ª—ñ—Å—Ç–∏—á–Ω–∞ —Ü–∏—Ç–∞—Ç–∞ —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é –º–æ–≤–æ—é, —è–∫–∞ –≤—ñ–¥–æ–±—Ä–∞–∂–∞—î —ó—Ö –¥—É–º–∫–∏, –±–æ–ª—ñ —Ç–∞ –º–æ—Ç–∏–≤–∞—Ü—ñ—é. 2-3 —Ä–µ—á–µ–Ω–Ω—è.]"
        
        ---
        
        –ö–†–ò–¢–ò–ß–ù–û –í–ê–ñ–õ–ò–í–û:
        - –ö–æ–∂–Ω–∞ –ø–µ—Ä—Å–æ–Ω–∞ –º–∞—î –±—É—Ç–∏ –†–Ü–ó–ù–û–Æ (—Ä—ñ–∑–Ω–∏–π –≤—ñ–∫, —Å—Ç–∞—Ç—å, –ø–æ—Ç—Ä–µ–±–∏)
        - –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π –£–ö–†–ê–á–ù–°–¨–ö–Ü —ñ–º–µ–Ω–∞
        - –ë—É–¥—å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ö–û–ù–ö–†–ï–¢–ù–ò–ú (–Ω–µ "25-45 —Ä–æ–∫—ñ–≤", –∞ "32-38 —Ä–æ–∫—ñ–≤")
        - –¶–∏—Ç–∞—Ç–∏ –º–∞—é—Ç—å –∑–≤—É—á–∞—Ç–∏ –ü–†–ò–†–û–î–ù–û —Ç–∞ –†–ï–ê–õ–Ü–°–¢–ò–ß–ù–û
        - –í—Ä–∞—Ö–æ–≤—É–π —Ç–∏–ø –±—ñ–∑–Ω–µ—Å—É ({biz_type})
        - –í—Å—è –≤—ñ–¥–ø–æ–≤—ñ–¥—å –£–ö–†–ê–á–ù–°–¨–ö–û–Æ –º–æ–≤–æ—é!
        """
        
        # print(f"Generating {num_personas} personas for {brand_name}...")
        response = self.ai_handler.generate_content(prompt)
        # print(f"Personas Generated (Length: {len(response.text)})")
        return response.text

    def generate_cjm(self, brand_name, industry, personas_text):
        """
        Generates a Customer Journey Map (CJM) in Markdown table format.
        """
        prompt = f"""
        –¢–∏ - –µ–∫—Å–ø–µ—Ä—Ç –∑ Customer Experience. –°—Ç–≤–æ—Ä–∏ Customer Journey Map (CJM) –¥–ª—è –±—Ä–µ–Ω–¥—É.
        
        –ë–†–ï–ù–î: {brand_name} ({industry})
        
        –ü–ï–†–°–û–ù–ò (–ê–£–î–ò–¢–û–†–Ü–Ø):
        {personas_text[:3000]}... (—Å–∫–æ—Ä–æ—á–µ–Ω–æ)
        
        –ó–ê–í–î–ê–ù–ù–Ø:
        –°—Ç–≤–æ—Ä–∏ CJM —É –≤–∏–≥–ª—è–¥—ñ Markdown —Ç–∞–±–ª–∏—Ü—ñ.
        
        –ï—Ç–∞–ø–∏ (–°—Ç–æ–≤–ø—Ü—ñ):
        1. –£—Å–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è (Awareness)
        2. –†–æ–∑–≥–ª—è–¥ (Consideration)
        3. –ü—Ä–∏–¥–±–∞–Ω–Ω—è (Purchase)
        4. –£—Ç—Ä–∏–º–∞–Ω–Ω—è (Retention)
        5. –ê–¥–≤–æ–∫–∞—Ü—ñ—è (Advocacy)
        
        –í–∏–º—ñ—Ä–∏ (–†—è–¥–∫–∏):
        - –¶—ñ–ª—ñ –∫–ª—ñ—î–Ω—Ç–∞ (User Goals)
        - –¢–æ—á–∫–∏ –∫–æ–Ω—Ç–∞–∫—Ç—É (Touchpoints)
        - –ï–º–æ—Ü—ñ—ó (Emotions - –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π –µ–º–æ–¥–∑—ñ)
        - –ë–∞—Ä'—î—Ä–∏ (Barriers)
        - –ú–æ–∂–ª–∏–≤–æ—Å—Ç—ñ –¥–ª—è –±—Ä–µ–Ω–¥—É (Opportunities)
        
        –í–ê–ñ–õ–ò–í–û:
        - –í—ñ–¥–ø–æ–≤—ñ–¥—å –º–∞—î –º—ñ—Å—Ç–∏—Ç–∏ –¢–Ü–õ–¨–ö–ò Markdown —Ç–∞–±–ª–∏—Ü—é.
        - –ú–æ–≤–∞: –£–ö–†–ê–á–ù–°–¨–ö–ê.
        - –ë—É–¥—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∏–º –¥–ª—è —Ü—ñ—î—ó –Ω—ñ—à—ñ.
        """
        
        # print(f"Generating CJM for {brand_name}...")
        response = self.ai_handler.generate_content(prompt)
        # print(f"CJM Generated (Length: {len(response.text)})")
        return response.text

    def analyze_competitor_tov(self, url):
        """
        Analyzes a competitor's website to extract their Tone of Voice.
        Returns a JSON object with emotional_tone, formality_level, unique_trait, values.
        """
        # print(f"Analyzing Competitor ToV: {url}")
        text_content = ""
        
        # 1. Scrape Content (Robust Method)
        try:
            with sync_playwright() as p:
                try:
                    browser = p.chromium.launch(headless=True)
                except:
                    os.system("playwright install chromium")
                    browser = p.chromium.launch(headless=True)
                
                try:
                    page = browser.new_page()
                    page.goto(url, timeout=15000)
                    text_content = page.inner_text('body')[:10000] # Get more text for analysis
                    page.close()
                except Exception as e:
                    # print(f"Playwright failed for {url}: {e}")
                    # Fallback to Requests
                    try:
                        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
                        resp = requests.get(url, headers=headers, timeout=10)
                        soup = BeautifulSoup(resp.content, 'html.parser')
                        text_content = soup.get_text(separator=' ', strip=True)[:10000]
                    except Exception as e2:
                        # print(f"Requests fallback failed: {e2}")
                        return {"error": "Could not scrape website"}
                finally:
                    browser.close()
        except Exception as e:
            # print(f"Scraping crashed: {e}")
            return {"error": str(e)}

        if not text_content:
            return {"error": "Empty content"}

        # 2. Analyze with AI
        prompt = f"""
        Analyze the text from a competitor's website and extract their Tone of Voice (ToV) characteristics.
        
        TEXT:
        {text_content[:5000]}...
        
        TASK:
        Return a JSON object with the following keys (values must be in UKRAINIAN):
        - "emotional_tone": Choose ONE from: ["–ù–µ–π—Ç—Ä–∞–ª—å–Ω–∏–π", "–î—Ä—É–∂–Ω—ñ–π", "–°–µ—Ä–π–æ–∑–Ω–∏–π", "–í–µ—Å–µ–ª–∏–π", "–ù–∞—Ç—Ö–Ω–µ–Ω–Ω–∏–π", "–ï–∫—Å–ø–µ—Ä—Ç–Ω–∏–π"]
        - "formality_level": Choose ONE from: ["–î—É–∂–µ –æ—Ñ—ñ—Ü—ñ–π–Ω–∏–π", "–û—Ñ—ñ—Ü—ñ–π–Ω–∏–π", "–°–µ—Ä–µ–¥–Ω—ñ–π", "–ù–µ–π—Ç—Ä–∞–ª—å–Ω–∏–π", "–î—Ä—É–∂–Ω—ñ–π", "–î—É–∂–µ –¥—Ä—É–∂–Ω—ñ–π"]
        - "unique_trait": (e.g., "–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è —Å–ª–µ–Ω–≥—É", "–ù–∞—É–∫–æ–≤–∏–π –ø—ñ–¥—Ö—ñ–¥", "–ì—É–º–æ—Ä", "–ú—ñ–Ω—ñ–º–∞–ª—ñ–∑–º")
        - "values": (list of 3 key values inferred from text)
        
        JSON ONLY. NO MARKDOWN.
        """
        
        response = self.ai_handler.generate_content(prompt)
        try:
            return json.loads(response.text.replace('```json', '').replace('```', ''))
        except:
            return {
                "emotional_tone": "–ù–µ –≤–∏–∑–Ω–∞—á–µ–Ω–æ",
                "formality_level": "–ù–µ –≤–∏–∑–Ω–∞—á–µ–Ω–æ",
                "unique_trait": "–ù–µ –≤–∏–∑–Ω–∞—á–µ–Ω–æ",
                "values": []
            }
