import os
import google.generativeai as genai
from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup
import json
import requests

class Strategist:
    def __init__(self, api_key):
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel('gemini-2.0-flash')

    # ... (previous methods) ...

    def analyze_competitors(self, urls):
        """
        Scrapes competitor URLs to extract outlines (H1-H3).
        """
        outlines = []
        
        # Try Playwright first
        try:
            with sync_playwright() as p:
                try:
                    browser = p.chromium.launch(headless=True)
                except Exception as e:
                    print(f"Playwright launch failed: {e}. Installing browsers...")
                    os.system("playwright install chromium")
                    browser = p.chromium.launch(headless=True)
                
                for url in urls:
                    try:
                        page = browser.new_page()
                        page.goto(url, timeout=15000)
                        content = page.content()
                        soup = BeautifulSoup(content, 'html.parser')
                        
                        h1 = soup.find('h1').get_text().strip() if soup.find('h1') else "No H1"
                        headings = [h.get_text().strip() for h in soup.find_all(['h2', 'h3'])]
                        
                        outlines.append({
                            "url": url,
                            "h1": h1,
                            "structure": headings[:10] # Limit to top 10 headings
                        })
                        page.close()
                    except Exception as e:
                        print(f"Playwright failed for {url}: {e}")
                        # Fallback to Requests inside loop
                        self._scrape_fallback(url, outlines)
                browser.close()
                
        except Exception as e:
            print(f"Playwright crashed completely: {e}")
            # Fallback for all URLs if browser fails entirely
            for url in urls:
                 self._scrape_fallback(url, outlines)

        return outlines

    def _scrape_fallback(self, url, outlines_list):
        """Fallback scraper using requests."""
        try:
            print(f"Falling back to requests for {url}")
            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
            response = requests.get(url, headers=headers, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            h1 = soup.find('h1').get_text().strip() if soup.find('h1') else "No H1 (Requests)"
            headings = [h.get_text().strip() for h in soup.find_all(['h2', 'h3'])]
            
            outlines_list.append({
                "url": url,
                "h1": h1,
                "structure": headings[:10]
            })
        except Exception as e:
            print(f"Requests fallback failed for {url}: {e}")
            outlines_list.append({
                "url": url,
                "h1": "Error scraping",
                "structure": [f"Error: {str(e)}"]
            })

    def _generate_with_retry(self, prompt, max_retries=3):
        """Helper to generate content with retry logic for 429 errors."""
        for attempt in range(max_retries):
            try:
                return self.model.generate_content(prompt)
            except Exception as e:
                if "429" in str(e) or "Resource exhausted" in str(e):
                    if attempt < max_retries - 1:
                        wait_time = (2 ** attempt) + random.uniform(0, 1)
                        print(f"âš ï¸ Rate limit hit. Retrying in {wait_time:.1f}s...")
                        time.sleep(wait_time)
                        continue
                raise e

    def analyze_serp(self, topic):
        """
        Analyzes SERP for intent and features.
        """
        print(f"Analyzing SERP for: {topic}")
        results = []
        
        # Attempt 1: Playwright (Google) - Stealth Mode
        try:
            with sync_playwright() as p:
                try:
                    browser = p.chromium.launch(headless=True, args=["--disable-blink-features=AutomationControlled"])
                except Exception as e:
                    print(f"Playwright launch failed: {e}. Installing browsers...")
                    os.system("playwright install chromium")
                    browser = p.chromium.launch(headless=True, args=["--disable-blink-features=AutomationControlled"])
                
                context = browser.new_context(
                    user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
                )
                page = context.new_page()
                
                print(f"Attempt 1: Scraping Google for: {topic}")
                try:
                    page.goto(f"https://www.google.com/search?q={topic}", timeout=10000)
                    page.wait_for_load_state("domcontentloaded")
                    
                    links = page.query_selector_all('div.g h3') # Standard Google selector
                    for h3 in links[:5]:
                        try:
                            parent_a = h3.xpath('..')[0]
                            url = parent_a.get_attribute('href')
                            title = h3.inner_text()
                            if url and title and url.startswith('http'):
                                results.append({"url": url, "title": title})
                        except:
                            continue
                except Exception as e:
                    print(f"Google scrape failed: {e}")

                browser.close()
        except Exception as e:
            print(f"Playwright Google failed: {e}")

        # Attempt 2: Requests (DuckDuckGo HTML) - Very Robust Fallback
        if not results:
            print("Attempt 2: Using DuckDuckGo HTML (Requests)...")
            try:
                import requests
                headers = {
                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
                    "Referer": "https://duckduckgo.com/"
                }
                # DDG HTML version doesn't require JS
                resp = requests.post("https://html.duckduckgo.com/html/", data={'q': topic}, headers=headers, timeout=10)
                if resp.status_code == 200:
                    soup = BeautifulSoup(resp.text, 'html.parser')
                    # DDG HTML selectors
                    for link in soup.find_all('a', class_='result__a')[:5]:
                        url = link.get('href')
                        title = link.get_text(strip=True)
                        if url and title:
                            results.append({"url": url, "title": title})
            except Exception as e:
                print(f"DDG Requests failed: {e}")

        # Attempt 3: Simulation (Last Resort)
        if not results:
            print("All scraping failed. Returning empty list to trigger manual input or retry.")
            # Do NOT return fake data anymore, it confuses the user.
            # Let's return a specific error-like result so the UI can show a warning but not fake info.
            results = [] 


        # Analyze Intent with Gemini
        prompt = f"Analyze the search intent for the topic '{topic}' in the context of Ukrainian Google Search. Return JSON with keys: 'intent' (Informational/Commercial/Transactional - translate to Ukrainian), 'features' (list of likely SERP features e.g. 'Ð’Ñ–Ð´ÐµÐ¾', 'Ð¡Ð½Ñ–Ð¿ÐµÑ‚', 'ÐšÐ°Ñ€Ñ‚Ð¸Ð½ÐºÐ¸')."
        response = self._generate_with_retry(prompt)
        try:
            analysis = json.loads(response.text.replace('```json', '').replace('```', ''))
        except:
            analysis = {"intent": "Informational", "features": []}

        return {
            "topic": topic,
            "competitors": results,
            "intent": analysis.get("intent"),
            "serp_features": analysis.get("features")
        }

    def analyze_competitors(self, urls):
        """
        Scrapes competitor URLs to extract outlines (H1-H3).
        """
        outlines = []
        with sync_playwright() as p:
            try:
                browser = p.chromium.launch(headless=True)
            except Exception as e:
                print(f"Playwright launch failed: {e}. Installing browsers...")
                os.system("playwright install chromium")
                browser = p.chromium.launch(headless=True)
            for url in urls:
                try:
                    page = browser.new_page()
                    page.goto(url, timeout=15000)
                    content = page.content()
                    soup = BeautifulSoup(content, 'html.parser')
                    
                    h1 = soup.find('h1').get_text(strip=True) if soup.find('h1') else "No H1"
                    headings = []
                    for h in soup.find_all(['h2', 'h3']):
                        headings.append(f"{h.name.upper()}: {h.get_text(strip=True)}")
                    
                    outlines.append({
                        "url": url,
                        "h1": h1,
                        "structure": headings[:10] # Limit for brevity
                    })
                    page.close()
                except Exception as e:
                    print(f"Error scraping {url}: {e}")
            browser.close()
        return outlines

    def extract_entities(self, text_content):
        """
        Uses Gemini to extract entities.
        """
        prompt = f"Extract key entities (products, ingredients, brands, technical terms) from the following text. Return as a JSON list of strings.\n\nText: {text_content[:2000]}"
        response = self._generate_with_retry(prompt)
        try:
            return json.loads(response.text.replace('```json', '').replace('```', ''))
        except:
            return []

    def suggest_faq(self, topic):
        """
        Generates FAQ questions based on the topic.
        """
        prompt = f"Generate 5 relevant FAQ questions for the topic '{topic}' that users might ask on Google. Return as a JSON list of strings."
        response = self._generate_with_retry(prompt)
        try:
            return json.loads(response.text.replace('```json', '').replace('```', ''))
        except:
            return [f"What is {topic}?", f"How to use {topic}?"]

    def generate_tov(self, brand_name, industry, url=None, emotional_tone="ÐÐµÐ¹Ñ‚Ñ€Ð°Ð»ÑŒÐ½Ð¸Ð¹", formality_level="ÐÐµÐ¹Ñ‚Ñ€Ð°Ð»ÑŒÐ½Ð¸Ð¹", unique_trait="", uploaded_docs=None):
        """
        Generates a Tone of Voice description based on brand info and optional URL scraping.
        """
        context = ""
        if url:
            try:
                with sync_playwright() as p:
                    browser = p.chromium.launch(headless=True)
                    page = browser.new_page()
                    page.goto(url, timeout=15000)
                    # Get text from home page
                    text = page.inner_text('body')[:5000] # Limit text
                    context = f"ÐšÐ¾Ð½Ñ‚ÐµÐ½Ñ‚ ÑÐ°Ð¹Ñ‚Ñƒ:\n{text}\n\n"
                    browser.close()
            except Exception as e:
                print(f"ToV Scraping failed: {e}")
                context = "Could not scrape website. "
        
        # Add uploaded documents context
        if uploaded_docs:
            # No need to import parser here, text is already extracted in app.py
            docs_context = "\n\nÐ”Ð¾Ð´Ð°Ñ‚ÐºÐ¾Ð²Ñ– Ð¼Ð°Ñ‚ÐµÑ€Ñ–Ð°Ð»Ð¸ Ð¿Ñ€Ð¾ Ð±Ñ€ÐµÐ½Ð´:\n"
            for doc in uploaded_docs[:3]:  # Limit to 3 docs
                try:
                    # Check if content is already text (new optimization) or bytes (old way)
                    if doc.get('is_text', False):
                        text = doc['content']
                    else:
                        # Fallback for backward compatibility
                        from utils.document_parser import extract_text_from_document
                        text = extract_text_from_document(doc['content'], doc['type'])
                        
                    docs_context += f"\n--- {doc['name']} ---\n{text[:5000]}\n"  # Increased limit to 5000 chars
                except Exception as e:
                    print(f"Error parsing {doc['name']}: {e}")
            context += docs_context

        prompt = f"""
        Ð¢Ð¸ - ÐµÐºÑÐ¿ÐµÑ€Ñ‚ Ð· Ð±Ñ€ÐµÐ½Ð´Ð¸Ð½Ð³Ñƒ. Ð¡Ñ‚Ð²Ð¾Ñ€Ð¸ Ð´ÐµÑ‚Ð°Ð»ÑŒÐ½Ð¸Ð¹ Ð³Ð°Ð¹Ð´ Tone of Voice (Ð“Ð¾Ð»Ð¾Ñ Ð‘Ñ€ÐµÐ½Ð´Ñƒ) Ð´Ð»Ñ Ð±Ñ€ÐµÐ½Ð´Ñƒ.
        
        Ð’ÐÐ–Ð›Ð˜Ð’Ðž: Ð’ÑÑ Ð²Ñ–Ð´Ð¿Ð¾Ð²Ñ–Ð´ÑŒ ÐžÐ‘ÐžÐ’'Ð¯Ð—ÐšÐžÐ’Ðž Ð¼Ð°Ñ” Ð±ÑƒÑ‚Ð¸ Ð£ÐšÐ ÐÐ‡ÐÐ¡Ð¬ÐšÐžÐ® Ð¼Ð¾Ð²Ð¾ÑŽ!
        
        Ð†ÐÐ¤ÐžÐ ÐœÐÐ¦Ð†Ð¯ ÐŸÐ Ðž Ð‘Ð Ð•ÐÐ”:
        - ÐÐ°Ð·Ð²Ð° Ð±Ñ€ÐµÐ½Ð´Ñƒ: {brand_name}
        - Ð†Ð½Ð´ÑƒÑÑ‚Ñ€Ñ–Ñ: {industry}
        - Ð•Ð¼Ð¾Ñ†Ñ–Ð¹Ð½Ð¸Ð¹ Ñ‚Ð¾Ð½: {emotional_tone}
        - Ð Ñ–Ð²ÐµÐ½ÑŒ Ñ„Ð¾Ñ€Ð¼Ð°Ð»ÑŒÐ½Ð¾ÑÑ‚Ñ–: {formality_level}
        - Ð£Ð½Ñ–ÐºÐ°Ð»ÑŒÐ½Ð° Ñ€Ð¸ÑÐ°: {unique_trait if unique_trait else "Ð½Ðµ Ð²ÐºÐ°Ð·Ð°Ð½Ð¾"}
        {context}
        
        Ð¡Ñ‚Ð²Ð¾Ñ€Ð¸ Markdown Ð³Ð°Ð¹Ð´, ÑÐºÐ¸Ð¹ Ð²ÐºÐ»ÑŽÑ‡Ð°Ñ”:
        
        ## 1. ÐžÑÐ½Ð¾Ð²Ð½Ñ– Ñ†Ñ–Ð½Ð½Ð¾ÑÑ‚Ñ–
        - 3-5 ÐºÐ»ÑŽÑ‡Ð¾Ð²Ð¸Ñ… Ñ†Ñ–Ð½Ð½Ð¾ÑÑ‚ÐµÐ¹ Ð±Ñ€ÐµÐ½Ð´Ñƒ (Ð¼Ð°Ñ€ÐºÐ¾Ð²Ð°Ð½Ð¸Ð¹ ÑÐ¿Ð¸ÑÐ¾Ðº)
        
        ## 2. Ð¥Ð°Ñ€Ð°ÐºÑ‚ÐµÑ€Ð¸ÑÑ‚Ð¸ÐºÐ¸ Ð³Ð¾Ð»Ð¾ÑÑƒ
        - 5-7 ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ð¸Ñ… Ñ…Ð°Ñ€Ð°ÐºÑ‚ÐµÑ€Ð¸ÑÑ‚Ð¸Ðº (Ð¼Ð°Ñ€ÐºÐ¾Ð²Ð°Ð½Ð¸Ð¹ ÑÐ¿Ð¸ÑÐ¾Ðº)
        - Ð’Ñ€Ð°Ñ…Ð¾Ð²ÑƒÐ¹ Ð¾Ð±Ñ€Ð°Ð½Ð¸Ð¹ ÐµÐ¼Ð¾Ñ†Ñ–Ð¹Ð½Ð¸Ð¹ Ñ‚Ð¾Ð½ ({emotional_tone}) Ñ‚Ð° Ñ„Ð¾Ñ€Ð¼Ð°Ð»ÑŒÐ½Ñ–ÑÑ‚ÑŒ ({formality_level})
        
        ## 3. Ð©Ð¾ Ñ€Ð¾Ð±Ð¸Ñ‚Ð¸
        - 5+ ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ð¸Ñ… Ñ€ÐµÐºÐ¾Ð¼ÐµÐ½Ð´Ð°Ñ†Ñ–Ð¹ Ð´Ð»Ñ ÐºÐ¾Ð¼ÑƒÐ½Ñ–ÐºÐ°Ñ†Ñ–Ñ—
        
        ## 4. Ð§Ð¾Ð³Ð¾ ÐÐ• Ñ€Ð¾Ð±Ð¸Ñ‚Ð¸ (Ð§ÐµÑ€Ð²Ð¾Ð½Ñ– Ð¿Ñ€Ð°Ð¿Ð¾Ñ€Ñ†Ñ–)
        - 5+ ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ð¸Ñ… Ñ€ÐµÑ‡ÐµÐ¹, ÑÐºÐ¸Ñ… ÑÐ»Ñ–Ð´ ÑƒÐ½Ð¸ÐºÐ°Ñ‚Ð¸
        - Ð¤Ñ€Ð°Ð·Ð¸, ÑÐºÑ– ÐÐ†ÐšÐžÐ›Ð˜ Ð½Ðµ Ð²Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð¾Ð²ÑƒÐ²Ð°Ñ‚Ð¸
        
        ## 5. ÐŸÑ€Ð¸ÐºÐ»Ð°Ð´Ð¸ Ñ„Ñ€Ð°Ð· (Ð¼Ñ–Ð½Ñ–Ð¼ÑƒÐ¼ 10)
        Ð¡Ñ‚Ð²Ð¾Ñ€Ð¸ ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ñ– Ð¿Ñ€Ð¸ÐºÐ»Ð°Ð´Ð¸ Ð´Ð»Ñ Ñ€Ñ–Ð·Ð½Ð¸Ñ… ÑÐ¸Ñ‚ÑƒÐ°Ñ†Ñ–Ð¹:
        - Ð—Ð°Ð³Ð¾Ð»Ð¾Ð²ÐºÐ¸ Ð¿Ñ€Ð¾Ð´ÑƒÐºÑ‚Ñ–Ð² (2 Ð¿Ñ€Ð¸ÐºÐ»Ð°Ð´Ð¸)
        - ÐžÐ¿Ð¸ÑÐ¸ ÐºÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ñ–Ð¹ (2 Ð¿Ñ€Ð¸ÐºÐ»Ð°Ð´Ð¸)
        - Email Ñ€Ð¾Ð·ÑÐ¸Ð»ÐºÐ¸ (2 Ð¿Ñ€Ð¸ÐºÐ»Ð°Ð´Ð¸)
        - Ð¡Ð¾Ñ†Ð¼ÐµÑ€ÐµÐ¶Ñ– (2 Ð¿Ñ€Ð¸ÐºÐ»Ð°Ð´Ð¸)
        - ÐžÐ±Ñ€Ð¾Ð±ÐºÐ° Ð·Ð°Ð¿ÐµÑ€ÐµÑ‡ÐµÐ½ÑŒ (2 Ð¿Ñ€Ð¸ÐºÐ»Ð°Ð´Ð¸)
        
        ÐšÐ¾Ð¶ÐµÐ½ Ð¿Ñ€Ð¸ÐºÐ»Ð°Ð´ Ð¼Ð°Ñ” Ð±ÑƒÑ‚Ð¸ Ñƒ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ñ–:
        **Ð¡Ð¸Ñ‚ÑƒÐ°Ñ†Ñ–Ñ:** [Ð¾Ð¿Ð¸Ñ]
        **Ð¤Ñ€Ð°Ð·Ð°:** "[ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ð° Ñ„Ñ€Ð°Ð·Ð° ÑƒÐºÑ€Ð°Ñ—Ð½ÑÑŒÐºÐ¾ÑŽ]"
        
        ÐšÐ Ð˜Ð¢Ð˜Ð§ÐÐž Ð’ÐÐ–Ð›Ð˜Ð’Ðž: 
        - ÐÐ• Ð²Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð¾Ð²ÑƒÐ¹ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñ–! Ð¢Ñ–Ð»ÑŒÐºÐ¸ Ð¼Ð°Ñ€ÐºÐ¾Ð²Ð°Ð½i ÑÐ¿Ð¸ÑÐºÐ¸.
        - Ð’ÑÑ Ð²Ñ–Ð´Ð¿Ð¾Ð²Ñ–Ð´ÑŒ Ð¼Ð°Ñ” Ð±ÑƒÑ‚Ð¸ Ð£ÐšÐ ÐÐ‡ÐÐ¡Ð¬ÐšÐžÐ® Ð¼Ð¾Ð²Ð¾ÑŽ (Ð²ÐºÐ»ÑŽÑ‡Ð½Ð¾ Ð· Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²ÐºÐ°Ð¼Ð¸)!
        - Ð‘ÑƒÐ´ÑŒ Ð¼Ð°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒÐ½Ð¾ ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ð¸Ð¼, Ð½Ðµ Ð·Ð°Ð³Ð°Ð»ÑŒÐ½Ð¸Ð¼.
        - Ð’Ñ€Ð°Ñ…Ð¾Ð²ÑƒÐ¹ ÑƒÐ½Ñ–ÐºÐ°Ð»ÑŒÐ½Ñƒ Ñ€Ð¸ÑÑƒ Ð±Ñ€ÐµÐ½Ð´Ñƒ: {unique_trait if unique_trait else "ÑÑ‚Ð²Ð¾Ñ€Ð¸ ÑƒÐ½Ñ–ÐºÐ°Ð»ÑŒÐ½Ð¸Ð¹ ÑÑ‚Ð¸Ð»ÑŒ"}
        - Ð¯ÐºÑ‰Ð¾ Ñ” Ð´Ð¾Ð´Ð°Ñ‚ÐºÐ¾Ð²Ñ– Ð¼Ð°Ñ‚ÐµÑ€Ñ–Ð°Ð»Ð¸, ÐžÐ‘ÐžÐ’'Ð¯Ð—ÐšÐžÐ’Ðž Ð¿Ð¾ÑÐ¸Ð»Ð°Ð¹ÑÑ Ð½Ð° Ð½Ð¸Ñ… (Ð½Ð°Ð¿Ñ€Ð¸ÐºÐ»Ð°Ð´: "Ð—Ð³Ñ–Ð´Ð½Ð¾ Ð· Ð²Ð°ÑˆÐ¾ÑŽ ÑÑ‚Ñ€Ð°Ñ‚ÐµÐ³Ñ–Ñ”ÑŽ...", "Ð¯Ðº Ð·Ð°Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¾ Ð² Ð´Ð¾ÑÐ»Ñ–Ð´Ð¶ÐµÐ½Ð½Ñ–...").
        - Ð¯ÐºÑ‰Ð¾ Ð¼Ð°Ñ‚ÐµÑ€Ñ–Ð°Ð»Ð¸ ÑÑƒÐ¿ÐµÑ€ÐµÑ‡Ð°Ñ‚ÑŒ Ð¾Ð´Ð¸Ð½ Ð¾Ð´Ð½Ð¾Ð¼Ñƒ, Ð½Ð°Ð´Ð°Ð²Ð°Ð¹ Ð¿Ñ€Ñ–Ð¾Ñ€Ð¸Ñ‚ÐµÑ‚ Ð·Ð°Ð²Ð°Ð½Ñ‚Ð°Ð¶ÐµÐ½Ð¸Ð¼ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°Ð¼.
        """
        
        print(f"Generating ToV for {brand_name}...")
        response = self._generate_with_retry(prompt)
        print(f"ToV Generated (Length: {len(response.text)})")
        return response.text

    def refine_tov(self, current_tov, instructions):
        """
        Refines the existing ToV based on user instructions.
        """
        prompt = f"""
        Act as a Brand Strategist. Refine the following Tone of Voice guide based on the user's instructions.
        
        Current ToV:
        {current_tov}
        
        User Instructions:
        {instructions}
        
        Output the updated ToV in Markdown. Keep the same structure if possible, but apply the requested changes. Language: Ukrainian.
        """
        
        print(f"Refining ToV...")
        response = self._generate_with_retry(prompt)
        print(f"ToV Refined (Length: {len(response.text)})")
        return response.text

    def generate_audience(self, brand_name, industry, url=None, business_model="B2C", num_personas=2):
        """
        Generates detailed target audience personas with Jobs-to-be-Done framework.
        """
        context = ""
        if url:
            try:
                with sync_playwright() as p:
                    browser = p.chromium.launch(headless=True)
                    page = browser.new_page()
                    page.goto(url, timeout=15000)
                    text = page.inner_text('body')[:5000]
                    context = f"ÐšÐ¾Ð½Ñ‚ÐµÐ½Ñ‚ ÑÐ°Ð¹Ñ‚Ñƒ:\n{text}\n\n"
                    browser.close()
            except Exception as e:
                print(f"Audience scraping failed: {e}")
                context = ""

        # Extract business model type
        if "B2B" in business_model and "B2C" in business_model:
            biz_type = "B2B Ñ‚Ð° B2C"
        elif "B2B" in business_model:
            biz_type = "B2B (Ð±Ñ–Ð·Ð½ÐµÑ Ð´Ð»Ñ Ð±Ñ–Ð·Ð½ÐµÑÑƒ)"
        else:
            biz_type = "B2C (Ð±Ñ–Ð·Ð½ÐµÑ Ð´Ð»Ñ ÑÐ¿Ð¾Ð¶Ð¸Ð²Ð°Ñ‡Ñ–Ð²)"

        prompt = f"""
        Ð¢Ð¸ - ÐµÐºÑÐ¿ÐµÑ€Ñ‚ Ð· Ð¼Ð°Ñ€ÐºÐµÑ‚Ð¸Ð½Ð³Ñƒ. Ð¡Ñ‚Ð²Ð¾Ñ€Ð¸ {num_personas} Ð´ÐµÑ‚Ð°Ð»ÑŒÐ½Ñ– Ð¿ÐµÑ€ÑÐ¾Ð½Ð¸ Ð´Ð»Ñ Ð±Ñ€ÐµÐ½Ð´Ñƒ.
        
        Ð’ÐÐ–Ð›Ð˜Ð’Ðž: Ð’ÑÑ Ð²Ñ–Ð´Ð¿Ð¾Ð²Ñ–Ð´ÑŒ ÐžÐ‘ÐžÐ’'Ð¯Ð—ÐšÐžÐ’Ðž Ð¼Ð°Ñ” Ð±ÑƒÑ‚Ð¸ Ð£ÐšÐ ÐÐ‡ÐÐ¡Ð¬ÐšÐžÐ® Ð¼Ð¾Ð²Ð¾ÑŽ!
        
        Ð†ÐÐ¤ÐžÐ ÐœÐÐ¦Ð†Ð¯ ÐŸÐ Ðž Ð‘Ð Ð•ÐÐ”:
        - ÐÐ°Ð·Ð²Ð° Ð±Ñ€ÐµÐ½Ð´Ñƒ: {brand_name}
        - Ð†Ð½Ð´ÑƒÑÑ‚Ñ€Ñ–Ñ: {industry}
        - Ð¢Ð¸Ð¿ Ð±Ñ–Ð·Ð½ÐµÑÑƒ: {biz_type}
        {context}
        
        Ð¡Ñ‚Ð²Ð¾Ñ€Ð¸ {num_personas} Ð Ð†Ð—ÐÐ† Ð¿ÐµÑ€ÑÐ¾Ð½Ð¸. ÐšÐ¾Ð¶Ð½Ð° Ð¿ÐµÑ€ÑÐ¾Ð½Ð° Ð¼Ð°Ñ” Ð±ÑƒÑ‚Ð¸ Ð£ÐÐ†ÐšÐÐ›Ð¬ÐÐžÐ® Ñ‚Ð° ÐšÐžÐÐšÐ Ð•Ð¢ÐÐžÐ®.
        
        Ð”Ð›Ð¯ ÐšÐžÐ–ÐÐžÐ‡ ÐŸÐ•Ð Ð¡ÐžÐÐ˜ ÑÑ‚Ð²Ð¾Ñ€Ð¸ Ñ‚Ð°ÐºÐ¸Ð¹ Markdown Ñ€Ð¾Ð·Ð´Ñ–Ð»:
        
        ## ÐŸÐµÑ€ÑÐ¾Ð½Ð° [ÐÐ¾Ð¼ÐµÑ€]: [Ð£ÐºÑ€Ð°Ñ—Ð½ÑÑŒÐºÐµ Ñ–Ð¼'Ñ] - [Ð Ð¾Ð»ÑŒ/ÐŸÐ¾ÑÐ°Ð´Ð°]
        
        ### ðŸ‘¤ Ð”ÐµÐ¼Ð¾Ð³Ñ€Ð°Ñ„Ñ–Ñ
        - **Ð’Ñ–Ðº:** [ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ð¸Ð¹ Ð´Ñ–Ð°Ð¿Ð°Ð·Ð¾Ð½]
        - **Ð¡Ñ‚Ð°Ñ‚ÑŒ:** [ÑÑ‚Ð°Ñ‚ÑŒ]
        - **Ð›Ð¾ÐºÐ°Ñ†Ñ–Ñ:** [Ð¼Ñ–ÑÑ‚Ð¾/Ñ€ÐµÐ³Ñ–Ð¾Ð½ Ð£ÐºÑ€Ð°Ñ—Ð½Ð¸]
        - **ÐžÑÐ²Ñ–Ñ‚Ð°:** [Ñ€Ñ–Ð²ÐµÐ½ÑŒ Ð¾ÑÐ²Ñ–Ñ‚Ð¸]
        - **Ð”Ð¾Ñ…Ñ–Ð´:** [Ñ€Ñ–Ð²ÐµÐ½ÑŒ Ð´Ð¾Ñ…Ð¾Ð´Ñƒ]
        - **Ð¡Ñ–Ð¼ÐµÐ¹Ð½Ð¸Ð¹ ÑÑ‚Ð°Ð½:** [ÑÑ‚Ð°Ð½]
        
        ### ðŸ§  ÐŸÑÐ¸Ñ…Ð¾Ð³Ñ€Ð°Ñ„Ñ–Ñ
        - **Ð¦Ñ–Ð½Ð½Ð¾ÑÑ‚Ñ–:** [3-4 ÐºÐ»ÑŽÑ‡Ð¾Ð²Ñ– Ñ†Ñ–Ð½Ð½Ð¾ÑÑ‚Ñ–]
        - **Ð†Ð½Ñ‚ÐµÑ€ÐµÑÐ¸:** [Ñ…Ð¾Ð±Ñ–, Ð·Ð°Ñ…Ð¾Ð¿Ð»ÐµÐ½Ð½Ñ]
        - **Ð¡Ñ‚Ð¸Ð»ÑŒ Ð¶Ð¸Ñ‚Ñ‚Ñ:** [Ð¾Ð¿Ð¸Ñ Ð¿Ð¾Ð²ÑÑÐºÐ´ÐµÐ½Ð½Ð¾Ð³Ð¾ Ð¶Ð¸Ñ‚Ñ‚Ñ]
        - **Ð£Ð»ÑŽÐ±Ð»ÐµÐ½Ñ– Ð±Ñ€ÐµÐ½Ð´Ð¸:** [2-3 Ð±Ñ€ÐµÐ½Ð´Ð¸, ÑÐºÑ– Ð²Ð¾Ð½Ð¸ Ð»ÑŽÐ±Ð»ÑÑ‚ÑŒ]
        - **ÐœÐµÐ´Ñ–Ð°:** [Ð´Ðµ ÑˆÑƒÐºÐ°ÑŽÑ‚ÑŒ Ñ–Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ñ–ÑŽ - ÑÐ¾Ñ†Ð¼ÐµÑ€ÐµÐ¶Ñ–, Ð±Ð»Ð¾Ð³Ð¸, YouTube Ñ‚Ð¾Ñ‰Ð¾]
        
        ### ðŸŽ¯ Jobs-to-be-Done
        - **Ð¤ÑƒÐ½ÐºÑ†Ñ–Ð¾Ð½Ð°Ð»ÑŒÐ½Ð° Ñ€Ð¾Ð±Ð¾Ñ‚Ð°:** [Ñ‰Ð¾ ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ð¾ Ñ…Ð¾Ñ‡ÑƒÑ‚ÑŒ Ð·Ñ€Ð¾Ð±Ð¸Ñ‚Ð¸/Ð´Ð¾ÑÑÐ³Ñ‚Ð¸]
        - **Ð•Ð¼Ð¾Ñ†Ñ–Ð¹Ð½Ð° Ñ€Ð¾Ð±Ð¾Ñ‚Ð°:** [ÑÐº Ñ…Ð¾Ñ‡ÑƒÑ‚ÑŒ Ð²Ñ–Ð´Ñ‡ÑƒÐ²Ð°Ñ‚Ð¸ ÑÐµÐ±Ðµ]
        - **Ð¡Ð¾Ñ†Ñ–Ð°Ð»ÑŒÐ½Ð° Ñ€Ð¾Ð±Ð¾Ñ‚Ð°:** [ÑÐº Ñ…Ð¾Ñ‡ÑƒÑ‚ÑŒ Ð²Ð¸Ð³Ð»ÑÐ´Ð°Ñ‚Ð¸ Ð² Ð¾Ñ‡Ð°Ñ… Ñ–Ð½ÑˆÐ¸Ñ…]
        
        ### ðŸ’” Ð‘Ð¾Ð»Ñ– Ñ‚Ð° Ð±Ð°Ñ€'Ñ”Ñ€Ð¸
        - [Ð‘Ñ–Ð»ÑŒ 1: ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ð° Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ð°]
        - [Ð‘Ñ–Ð»ÑŒ 2: ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ð° Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ð°]
        - [Ð‘Ñ–Ð»ÑŒ 3: ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ð° Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ð°]
        - **Ð“Ð¾Ð»Ð¾Ð²Ð½Ð¸Ð¹ Ð±Ð°Ñ€'Ñ”Ñ€ Ð´Ð¾ Ð¿Ð¾ÐºÑƒÐ¿ÐºÐ¸:** [Ñ‰Ð¾ Ð·Ð°Ð²Ð°Ð¶Ð°Ñ” ÐºÑƒÐ¿Ð¸Ñ‚Ð¸]
        
        ### ðŸš€ Ð¢Ñ€Ð¸Ð³ÐµÑ€Ð¸ Ñ‚Ð° Ð¼Ð¾Ñ‚Ð¸Ð²Ð°Ñ‚Ð¾Ñ€Ð¸
        - **Ð©Ð¾ Ð·Ð¼ÑƒÑˆÑƒÑ” ÑˆÑƒÐºÐ°Ñ‚Ð¸ Ñ€Ñ–ÑˆÐµÐ½Ð½Ñ:** [ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ð° ÑÐ¸Ñ‚ÑƒÐ°Ñ†Ñ–Ñ]
        - **ÐšÑ€Ð¸Ñ‚ÐµÑ€Ñ–Ñ— Ð²Ð¸Ð±Ð¾Ñ€Ñƒ:** [Ñ‚Ð¾Ð¿-3 ÐºÑ€Ð¸Ñ‚ÐµÑ€Ñ–Ñ— Ð¿Ñ€Ð¸ Ð²Ð¸Ð±Ð¾Ñ€Ñ– Ð¿Ñ€Ð¾Ð´ÑƒÐºÑ‚Ñƒ]
        - **Ð©Ð¾ Ð²Ð¿Ð»Ð¸Ð²Ð°Ñ” Ð½Ð° Ñ€Ñ–ÑˆÐµÐ½Ð½Ñ:** [Ñ…Ñ‚Ð¾/Ñ‰Ð¾ Ð²Ð¿Ð»Ð¸Ð²Ð°Ñ” Ð½Ð° Ð²Ð¸Ð±Ñ–Ñ€]
        
        ### ðŸ›’ Customer Journey
        - **Ð£ÑÐ²Ñ–Ð´Ð¾Ð¼Ð»ÐµÐ½Ð½Ñ:** [ÑÐº Ð´Ñ–Ð·Ð½Ð°Ñ”Ñ‚ÑŒÑÑ Ð¿Ñ€Ð¾ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ñƒ]
        - **Ð Ð¾Ð·Ð³Ð»ÑÐ´:** [ÑÐº Ð¿Ð¾Ñ€Ñ–Ð²Ð½ÑŽÑ” Ð²Ð°Ñ€Ñ–Ð°Ð½Ñ‚Ð¸, Ñ‰Ð¾ Ð³ÑƒÐ³Ð»Ð¸Ñ‚ÑŒ]
        - **Ð Ñ–ÑˆÐµÐ½Ð½Ñ:** [Ñ‰Ð¾ Ð¾ÑÑ‚Ð°Ñ‚Ð¾Ñ‡Ð½Ð¾ Ð²Ð¿Ð»Ð¸Ð²Ð°Ñ” Ð½Ð° Ð²Ð¸Ð±Ñ–Ñ€]
        - **ÐŸÐ¾ÐºÑƒÐ¿ÐºÐ°:** [Ð´Ðµ Ñ‚Ð° ÑÐº ÐºÑƒÐ¿ÑƒÑ”]
        
        ### ðŸ’¬ Ð¦Ð¸Ñ‚Ð°Ñ‚Ð° Ð¿ÐµÑ€ÑÐ¾Ð½Ð¸
        "[Ð ÐµÐ°Ð»Ñ–ÑÑ‚Ð¸Ñ‡Ð½Ð° Ñ†Ð¸Ñ‚Ð°Ñ‚Ð° ÑƒÐºÑ€Ð°Ñ—Ð½ÑÑŒÐºÐ¾ÑŽ Ð¼Ð¾Ð²Ð¾ÑŽ, ÑÐºÐ° Ð²Ñ–Ð´Ð¾Ð±Ñ€Ð°Ð¶Ð°Ñ” Ñ—Ñ… Ð´ÑƒÐ¼ÐºÐ¸, Ð±Ð¾Ð»Ñ– Ñ‚Ð° Ð¼Ð¾Ñ‚Ð¸Ð²Ð°Ñ†Ñ–ÑŽ. 2-3 Ñ€ÐµÑ‡ÐµÐ½Ð½Ñ.]"
        
        ---
        
        ÐšÐ Ð˜Ð¢Ð˜Ð§ÐÐž Ð’ÐÐ–Ð›Ð˜Ð’Ðž:
        - ÐšÐ¾Ð¶Ð½Ð° Ð¿ÐµÑ€ÑÐ¾Ð½Ð° Ð¼Ð°Ñ” Ð±ÑƒÑ‚Ð¸ Ð Ð†Ð—ÐÐžÐ® (Ñ€Ñ–Ð·Ð½Ð¸Ð¹ Ð²Ñ–Ðº, ÑÑ‚Ð°Ñ‚ÑŒ, Ð¿Ð¾Ñ‚Ñ€ÐµÐ±Ð¸)
        - Ð’Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð¾Ð²ÑƒÐ¹ Ð£ÐšÐ ÐÐ‡ÐÐ¡Ð¬ÐšÐ† Ñ–Ð¼ÐµÐ½Ð°
        - Ð‘ÑƒÐ´ÑŒ Ð¼Ð°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒÐ½Ð¾ ÐšÐžÐÐšÐ Ð•Ð¢ÐÐ˜Ðœ (Ð½Ðµ "25-45 Ñ€Ð¾ÐºÑ–Ð²", Ð° "32-38 Ñ€Ð¾ÐºÑ–Ð²")
        - Ð¦Ð¸Ñ‚Ð°Ñ‚Ð¸ Ð¼Ð°ÑŽÑ‚ÑŒ Ð·Ð²ÑƒÑ‡Ð°Ñ‚Ð¸ ÐŸÐ Ð˜Ð ÐžÐ”ÐÐž Ñ‚Ð° Ð Ð•ÐÐ›Ð†Ð¡Ð¢Ð˜Ð§ÐÐž
        - Ð’Ñ€Ð°Ñ…Ð¾Ð²ÑƒÐ¹ Ñ‚Ð¸Ð¿ Ð±Ñ–Ð·Ð½ÐµÑÑƒ ({biz_type})
        - Ð’ÑÑ Ð²Ñ–Ð´Ð¿Ð¾Ð²Ñ–Ð´ÑŒ Ð£ÐšÐ ÐÐ‡ÐÐ¡Ð¬ÐšÐžÐ® Ð¼Ð¾Ð²Ð¾ÑŽ!
        """
        
        print(f"Generating {num_personas} personas for {brand_name}...")
        response = self.model.generate_content(prompt)
        print(f"Personas Generated (Length: {len(response.text)})")
        return response.text
